---
sidebar: sidebar
permalink: ai/oai_bpod_architecture.html
keywords: NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NVIDIA AI Enterprise, NVIDIA BasePOD, NVIDIA DGX
summary: ONTAP AI with NVIDIA DGX BasePOD - Sizing Guidance
---

= ONTAP AI with NVIDIA DGX BasePOD - Sizing Guidance
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./../media/

link:oai_bpod_architecture.html[Previous: Architecture.]

== Sizing Guidance

This architecture is intended as a reference for customers and partners who would like to implement a DL infrastructure with NVIDIA DGX A100 systems and NetApp AFF storage systems. 
As demonstrated in previous versions of this reference architecture, the AFF A800 system easily supports the DL training workload generated by eight DGX A100 systems. For larger deployments with higher storage performance requirements, additional AFF A800 systems can be added to the NetApp ONTAP cluster. ONTAP 9 supports up to 12 HA pairs (24 nodes) in a single cluster. With the FlexGroup technology validated in this solution, a 24-node cluster can provide over 20 PB and up to 300 GBps throughput in a single volume. Other NetApp storage systems such as the AFF A400 and A250 offer lower performance and capacity options for smaller deployments at lower cost points. Based on the results of this testing, an AFF A400 storage system can support one or two DGX A100 systems with the workloads that were tested. Because ONTAP 9 supports mixed-model clusters, customers can start with a smaller initial footprint and add more or larger storage systems to the cluster as capacity and performance requirements grow. The table below shows a rough estimate of the number of A100 GPUs supported on each AFF A-series model.

image:oai_sizing.png[Error: Missing Graphic Image]

link:oai_bpod_conclusion.html[Next: Conclusion.]